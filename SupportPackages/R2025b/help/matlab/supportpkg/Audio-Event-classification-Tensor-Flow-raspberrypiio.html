<!DOCTYPE HTML>
<html xmlns="http://www.w3.org/1999/xhtml" itemscope="" itemtype="http://www.mathworks.com/help/schema/MathWorksDocPage">
<head>
<meta xmlns="http://www.w3.org/1999/xhtml" charset="utf-8">
<meta xmlns="http://www.w3.org/1999/xhtml" name="viewport" content="width=device-width, initial-scale=1.0">
<meta xmlns="http://www.w3.org/1999/xhtml" http-equiv="X-UA-Compatible" content="IE=edge">
<title>Audio Event Classification Using TensorFlow Lite on Raspberry Pi</title>
<script xmlns="http://www.w3.org/1999/xhtml" type="application/ld+json">
      {
      "@context": "http://schema.org",
      "@type": "BreadcrumbList",
      "itemListElement":
      [{
          "@type": "ListItem",
          "position": 1,

          "item": {
          "@id": "../index.html",
          "name": "MATLAB"
}

          } 
        ,
        {
          "@type": "ListItem",
          "position": 2,

          "item": {
          "@id": "../data-import-and-analysis.html",
          "name": "Data Import and Analysis"
}

          }
        ,
        {
          "@type": "ListItem",
          "position": 3,

          "item": {
          "@id": "../data-import-and-export.html",
          "name": "Data Import and Export"
}

          }
        ,
        {
          "@type": "ListItem",
          "position": 4,

          "item": {
          "@id": "../hardware-network-comm.html",
          "name": "Hardware and Network Communication"
}

          }
        ,
        {
          "@type": "ListItem",
          "position": 5,

          "item": {
          "@id": "../hardware-boards-and-kits.html",
          "name": "Hardware Boards and Kits"
}

          }
        ,
        {
          "@type": "ListItem",
          "position": 6,

          "item": {
          "@id": "../raspberrypiio.html",
          "name": "Raspberry Pi Hardware"
}

          }
        ,
        {
          "@type": "ListItem",
          "position": 7,

          "item": {
          "@id": "../raspberrypiio-audio.html",
          "name": "Audio"
}

          }]
      }</script><script xmlns="http://www.w3.org/1999/xhtml" type="application/ld+json">
        {
        "@context": "http://schema.org",
        "@type": "ItemList",
          "name": "VisibleBreadcrumbs",

        "itemListElement":
        [
        "raspberrypiio-audio"
        ],
        "itemListOrder": "http://schema.org/ItemListOrderAscending"
        }
        </script><script xmlns="http://www.w3.org/1999/xhtml" type="application/ld+json">
        {
        "@context": "http://schema.org",
        "@type": "DigitalDocument",
          "headline": "Audio Event Classification Using TensorFlow Lite on Raspberry Pi",
          "description": "Demonstrates audio event classification using a pretrained deep neural network, YAMNet, from TensorFlow&#8482; Lite library on Raspberry Pi&#8482;. You load the TensorFlow Lite model and predict the class for the given audio frame on Raspberry Pi using a processor-in-the-loop (PIL) workflow. To generate code on Raspberry Pi, you use Embedded Coder&#174;, MATLAB&#174; Support Package for Raspberry Pi Hardware and Deep Learning Toolbox Interface for TensorFlow Lite. Refer to Audio Classification and yamnet classification for more details on the YAMNet model description.",
          "thumbnailURL": "../../examples/deeplearning_shared/win64/AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample_01.png",
          "genre": "Live Script",
          "isBasedOn": {
          "@type": "Product",
          "name": "MATLAB"

        },
          "identifier": "deeplearning_shared.AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample",
          "name": "AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample",
          "url": "Audio-Event-classification-Tensor-Flow-raspberrypiio.html"

        }</script><script xmlns="http://www.w3.org/1999/xhtml" type="application/ld+json">
        {
        "@context": "http://schema.org",
        "@type": "PropertyValue",
          "name": "open_command",
          "value": "matlab:openExample('deeplearning_shared/AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample')"

        }</script><script xmlns="http://www.w3.org/1999/xhtml" type="application/ld+json">
        {
        "@context": "http://schema.org",
        "@type": "ItemList",
          "name": "ExampleSourceFiles",

        "itemListElement":
        [
        "AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample.mlx",
        "predictAudioClassUsingYAMNET.m"
        ],
        "itemListOrder": "http://schema.org/ItemListOrderAscending"
        }
        </script><link xmlns="http://www.w3.org/1999/xhtml" href="../../includes/product/css/bootstrap.min.css" rel="stylesheet" type="text/css">


  <meta xmlns="http://www.w3.org/1999/xhtml" http-equiv="Content-Script-Type" content="text/javascript">
<meta xmlns="http://www.w3.org/1999/xhtml" name="toctype" itemprop="pagetype" content="ug">
<meta xmlns="http://www.w3.org/1999/xhtml" name="infotype" itemprop="infotype" content="other">

<meta xmlns="http://www.w3.org/1999/xhtml" name="description" itemprop="description" content="This example demonstrates audio event classification using a pretrained deep neural network, YAMNet, from TensorFlow&#8482; Lite library on Raspberry Pi&#8482;."><meta xmlns="http://www.w3.org/1999/xhtml" content="../../examples/deeplearning_shared/win64/AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample_01.png" itemprop="thumbnailUrl">
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/product/scripts/jquery/jquery-3.6.0.min.js"></script><script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/product/scripts/jquery/jquery-migrate.min.js"></script>
<link xmlns="http://www.w3.org/1999/xhtml" href="../../includes/product/css/site6.css" rel="stylesheet" type="text/css">
<link xmlns="http://www.w3.org/1999/xhtml" href="../../includes/product/css/site6_lg.css" rel="stylesheet" media="screen and (min-width: 1200px)">
<link xmlns="http://www.w3.org/1999/xhtml" href="../../includes/product/css/site6_md.css" rel="stylesheet" media="screen and (min-width: 992px) and (max-width: 1199px)">
<link xmlns="http://www.w3.org/1999/xhtml" href="../../includes/product/css/site6_sm+xs.css" rel="stylesheet" media="screen and (max-width: 991px)">
<link xmlns="http://www.w3.org/1999/xhtml" href="../../includes/product/css/site6_sm.css" rel="stylesheet" media="screen and (min-width: 768px) and (max-width: 991px)">
<link xmlns="http://www.w3.org/1999/xhtml" href="../../includes/product/css/site6_xs.css" rel="stylesheet" media="screen and (max-width: 767px)">
<link xmlns="http://www.w3.org/1999/xhtml" href="../../includes/product/css/site6_offcanvas_v2.css" rel="stylesheet" type="text/css">
<link xmlns="http://www.w3.org/1999/xhtml" href="../../includes/shared/highlight/styles/mwdochighlight.min.css" rel="stylesheet" type="text/css">

<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/shared/scripts/l10n.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/shared/scripts/docscripts.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/shared/scripts/f1help.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/product/scripts/docscripts.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/shared/scripts/mw.imageanimation.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/shared/scripts/jquery.highlight.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/product/scripts/underscore-min.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/product/scripts/use_platform_screenshots.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/product/scripts/suggest.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/product/scripts/overload.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/shared/scripts/helpservices.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/product/scripts/productfilter.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/shared/scripts/matlab_dialog_shared.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/shared/highlight/highlight.min.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/shared/scripts/localstorage.js"></script><script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/shared/scripts/product_group.js"></script><script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/shared/scripts/saxonjs/SaxonJS2.rt.js"></script><script xmlns="http://www.w3.org/1999/xhtml">
            window.history.replaceState(window.location.href, null, ""); // Initialize
            window.onload = function() {    
            mystylesheetLocation = "../../includes/shared/scripts/product_group-sef.json";
            mysourceLocation = "../../docset/docset.xml";
            product_help_location = "matlab";
            pagetype = "section";
            doccentertype = "product";
            langcode = "";
            getProductFilteredList(mystylesheetLocation, mysourceLocation, product_help_location, pagetype, doccentertype, langcode);  
            }
          </script><script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/shared/scripts/livecontrolpopover.js"></script>





<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/product/scripts/jquery/jquery.mobile.custom.min.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/product/scripts/bootstrap.bundle.min.js"></script>
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/product/scripts/global.js"></script>
<link xmlns="http://www.w3.org/1999/xhtml" href="../../includes/product/css/doc_center_base.css" rel="stylesheet" type="text/css">
<link xmlns="http://www.w3.org/1999/xhtml" href="../../includes/product/css/doc_center_installed.css" rel="stylesheet" type="text/css">
<link xmlns="http://www.w3.org/1999/xhtml" href="../../includes/product/css/doc_center_print.css" rel="stylesheet" type="text/css" media="print">
<script xmlns="http://www.w3.org/1999/xhtml" src="../../includes/shared/equationrenderer/release/MathRenderer.js"></script>
</head>
<body id="responsive_offcanvas">
<div xmlns="http://www.w3.org/1999/xhtml" id="doc_header_spacer" class="header"></div>
<div xmlns="http://www.w3.org/1999/xhtml" class="section_header level_3"><div class="container-fluid"><div class="row" id="mobile_search_row"><div class="col-sm-6 col-md-7 has_horizontal_local_nav" id="section_header_title"><div class="section_header_content"><div class="section_header_title"><h1><a href="../../documentation-center.html">Help Center</a></h1></div></div></div><div class="col-12 col-sm-6 col-md-5" id="mobile_search"><div class="search_nested_content_container"><form id="docsearch_form" name="docsearch_form" method="get" data-release="R2025b" data-language="en" action="../../templates/searchresults.html"><div class="input-group tokenized_search_field"><label class="visually-hidden form-label">Search Help</label><input type="text" class="form-control conjoined_search" autocomplete="off" name="qdoc" placeholder="Search Help" id="docsearch"> <div><button type="submit" name="submitsearch" id="submitsearch" class="btn icon-search btn_search_adjacent btn_search icon_16" tabindex="-1"></button></div></div></form></div><button class="btn icon-remove btn_search float-end icon_32 d-sm-none" data-bs-toggle="collapse" href="#mobile_search" aria-expanded="false" aria-controls="mobile_search"></button></div><div class="d-sm-none" id="search_actuator"><button class="btn icon-search btn_search float-end icon_16" data-bs-toggle="collapse" href="#mobile_search" aria-expanded="false" aria-controls="mobile_search"></button></div></div></div></div><div class="row-offcanvas row-offcanvas-left">
<div xmlns="http://www.w3.org/1999/xhtml" class="sidebar-offcanvas" id="sidebar">
<nav class="offcanvas_nav" role="navigation">
<div class="offcanvas_actuator" data-bs-toggle="offcanvas" data-bs-target="#sidebar" id="nav_toggle"><button type="button" class="btn"><span class="visually-hidden">Off-Canvas Navigation Menu Toggle
                  Off-Canvas Navigation Menu Toggle</span><span class="icon-menu"></span></button><span class="offcanvas_actuator_label" id="translation_icon-menu" tabindex="-1" aria-hidden="true"></span></div><div class="nav_list_wrapper" id="nav_list_wrapper"><nav class="offcanvas_nav" role="navigation"><ul class="nav_breadcrumb" id="ul_left_nav_ancestors"><li itemscope="" itemtype="http://www.data-vocabulary.org/Breadcrumb" itemprop="breadcrumb"><a href="../../documentation-center.html?s_tid=CRUX_lftnav" itemprop="url"><span itemprop="title">Documentation Home</span></a></li></ul>
<ul class="nav_disambiguation"><li><a href="../index.html?s_tid=CRUX_lftnav" id="index">MATLAB</a>
</li>
<li itemscope="" itemtype="http://www.data-vocabulary.org/Breadcrumb" itemprop="breadcrumb"><a href="../data-import-and-analysis.html?s_tid=CRUX_lftnav" itemprop="url"><span itemprop="title">Data Import and Analysis</span></a></li><li itemscope="" itemtype="http://www.data-vocabulary.org/Breadcrumb" itemprop="breadcrumb"><a href="../data-import-and-export.html?s_tid=CRUX_lftnav" itemprop="url"><span itemprop="title">Data Import and Export</span></a></li><li itemscope="" itemtype="http://www.data-vocabulary.org/Breadcrumb" itemprop="breadcrumb"><a href="../hardware-network-comm.html?s_tid=CRUX_lftnav" itemprop="url"><span itemprop="title">Hardware and Network Communication</span></a></li><li itemscope="" itemtype="http://www.data-vocabulary.org/Breadcrumb" itemprop="breadcrumb"><a href="../hardware-boards-and-kits.html?s_tid=CRUX_lftnav" itemprop="url"><span itemprop="title">Hardware Boards and Kits</span></a></li><li itemscope="" itemtype="http://www.data-vocabulary.org/Breadcrumb" itemprop="breadcrumb"><a href="../raspberrypiio.html?s_tid=CRUX_lftnav" itemprop="url"><span itemprop="title">Raspberry Pi Hardware</span></a></li><li itemscope="" itemtype="http://www.data-vocabulary.org/Breadcrumb" itemprop="breadcrumb"><a href="../raspberrypiio-audio.html?s_tid=CRUX_lftnav" itemprop="url" id="mw_20b23641-0231-4308-a092-206d14e343be"><span itemprop="title">Audio</span></a></li></ul><div class="search_refine_v4"><div id="facets_area"><ul class="nav_scrollspy nav list-unstyled" id="pnav">
<li class="nav_scrollspy_function nav-item"><a href="#responsive_offcanvas">Audio Event Classification Using TensorFlow Lite on Raspberry Pi</a></li>
<li class="nav_scrollspy_title nav-item" id="SSPY810-section">On this page</li>
<!--ADD_REFENTRY_TITLE_HERE 11--><li class="nav-item"><a href="#AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-1" class="nav-link intrnllnk">Third-Party Prerequisites</a></li><li class="nav-item"><a href="#AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-2" class="nav-link intrnllnk">Download YAMNet</a></li><li class="nav-item"><a href="#AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-3" class="nav-link intrnllnk">Read Audio Data and Classify the Sounds</a></li><li class="nav-item"><a href="#AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-4" class="nav-link intrnllnk">Load TensorFlow Lite Model and Audio Event Classes</a></li><li class="nav-item"><a href="#AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-5" class="nav-link intrnllnk">Read Input Audio</a></li><li class="nav-item"><a href="#AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-6" class="nav-link intrnllnk">Setup the FIFO Buffers</a></li><li class="nav-item"><a href="#AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-7" class="nav-link intrnllnk">Run TFLite YAMNet in MATLAB to Perform Audio Event Classification</a></li><li class="nav-item"><a href="#AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-8" class="nav-link intrnllnk">Prepare MATLAB Code for Deployment</a></li><li class="nav-item"><a href="#AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-9" class="nav-link intrnllnk">Generate Code for Audio Event Classifier on Raspberry Pi</a></li><li class="nav-item"><a href="#AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-10" class="nav-link intrnllnk">Predict Audio Class on Raspberry Pi Using PIL Workflow</a></li><li class="nav-item"><a href="#AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-11" class="nav-link intrnllnk">Evaluate Raspberry Pi Execution Time</a></li></ul></div></div>
</nav></div></nav>
<script src="../../includes/product/scripts/offcanvas_v2.js"></script></div><!--END.CLASS sidebar-offcanvas-->
<div class="offcanvas_content_container">
<div xmlns="http://www.w3.org/1999/xhtml" class="sticky_header_container"><div class="horizontal_nav"><div class="horizontal_nav_container"><div class="offcanvas_horizontal_nav"><div class="container-fluid"><div class="row"><div class="col-sm-12 d-none d-sm-block"><nav class="navbar navbar-default" role="navigation" id="subnav"><div><ul class="nav navbar-nav crux_browse"><li id="crux_nav_documentation" class="crux_resource active">Documentation</li><li id="crux_nav_example" class="crux_resource"><a href="../examples.html?category=raspberrypiio-audio&amp;s_tid=CRUX_topnav">Examples</a></li><li id="crux_nav_function" class="crux_resource"><a href="../referencelist.html?type=function&amp;category=raspberrypiio-audio&amp;s_tid=CRUX_topnav">Functions</a></li><li id="crux_nav_app" class="crux_resource"><a href="../referencelist.html?type=app&amp;category=raspberrypiio-audio&amp;s_tid=CRUX_topnav">Apps</a></li></ul></div></nav></div><div class="d-sm-none"><div class="container-fluid"><div class="row"><div class="col-9"><div class="mobile_crux_nav_trigger"><div class="btn-group"><button type="button" class="btn btn-default dropdown-toggle" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Resources</button><ul class="dropdown-menu"><li id="crux_nav_mobile_documentation" class="crux_resource active">Documentation</li><li id="crux_nav_mobile_example" class="crux_resource"><a href="../examples.html?category=raspberrypiio-audio&amp;s_tid=CRUX_topnav">Examples</a></li><li id="crux_nav_mobile_function" class="crux_resource"><a href="../referencelist.html?type=function&amp;category=raspberrypiio-audio&amp;s_tid=CRUX_topnav">Functions</a></li><li id="crux_nav_mobile_app" class="crux_resource"><a href="../referencelist.html?type=app&amp;category=raspberrypiio-audio&amp;s_tid=CRUX_topnav">Apps</a></li></ul></div></div></div><div class="col-3"><div class="translate_placeholder"></div></div></div></div></div></div></div></div></div></div></div><div class="content_container" id="content_container" itemprop="content">
<div class="container-fluid">
<div class="row">
<div class="col-12">

<main id="skip_link_anchor" tabindex="-1">
<div xmlns="http://www.w3.org/1999/xhtml" id="product_info_alert"></div><section xmlns="http://www.w3.org/1999/xhtml" id="doc_center_content" lang="en" data-language="en"><div id="pgtype-topic">
<section><h1 class="r2025b" itemprop="title content" id="mw_e14fe92a-b70f-4a81-8f17-1d7a08a20109">Audio Event Classification Using TensorFlow Lite on Raspberry Pi</h1><div class="examples_short_list hidden_ios_android" data-products="ML AU DL_TENSORFLOW_LITE NN EC"><div data-pane="metadata" class="card metadata_container"><div class="card-body metadata_content"><p class="add_margin_5">This example uses:</p><ul class="list-unstyled add_border_bottom example_product_list" itemprop="requiredprods"><li><a href="matlab:matlab.internal.addons.showAddon('AU')">Audio Toolbox</a></li><li><a href="matlab:matlab.internal.addons.showAddon('DL_TENSORFLOW_LITE')">Deep Learning Toolbox Interface for TensorFlow Lite</a></li><li><a href="matlab:matlab.internal.addons.showAddon('NN')">Deep Learning Toolbox</a></li><li><a href="matlab:matlab.internal.addons.showAddon('EC')">Embedded Coder</a></li></ul><div class="d-grid"><a class="btn btn_color_blue" href="matlab:openExample('deeplearning_shared/AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample')" data-ex-genre="Live Script">Open Live Script</a></div></div></div></div><div itemscope="" itemtype="http://www.mathworks.com/help/schema/MathWorksDocPage/Example" itemprop="example" class="em_example"><meta itemprop="exampleid" content="deeplearning_shared-AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample"><meta itemprop="exampletitle" content="Audio Event Classification Using TensorFlow Lite on Raspberry Pi"></div><span id="AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample" class="anchor_target"></span><p class="shortdesc">This example demonstrates audio event classification using a pretrained deep neural network, YAMNet, from TensorFlow&#8482; Lite library on Raspberry Pi&#8482;. You load the TensorFlow Lite model and predict the class for the given audio frame on Raspberry Pi using a processor-in-the-loop (PIL) workflow. To generate code on Raspberry Pi, you use Embedded Coder&#174;, MATLAB&#174; Support Package for Raspberry Pi Hardware and Deep Learning Toolbox Interface for TensorFlow Lite. Refer to <a href="https://www.tensorflow.org/lite/examples/audio_classification/overview" target="_blank">Audio Classification</a> and <a href="https://tfhub.dev/google/lite-model/yamnet/classification/tflite/1" target="_blank">yamnet classification</a> for more details on the YAMNet model description.  </p><div class="procedure"><h3 class="title" id="AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-1"><strong class="emphasis bold">Third-Party Prerequisites</strong></h3><div class="itemizedlist"><ul><li><p>Raspberry Pi hardware</p></li><li><p>TensorFlow Lite library (on the target ARM&#174; hardware)</p></li><li><p>Pretrained TensorFlow Lite Model</p></li></ul></div><h3 class="title" id="AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-2">Download YAMNet</h3><p>Download and unzip the <a href="../../audio/ref/yamnet.html" data-docid="audio_ref#function_yamnet" class="a"><code class="olink">yamnet</code></a><span role="cross_prod"> (Audio Toolbox)</span>.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>component = <span style="color:#A020F0">"audio"</span>;
filename = <span style="color:#A020F0">"yamnet.zip"</span>;
localfile = matlab.internal.examples.downloadSupportFile(component,filename);
downloadFolder = fileparts(localfile);
<span style="color:#0000FF">if</span> exist(fullfile(downloadFolder,<span style="color:#A020F0">"yamnet"</span>),<span style="color:#A020F0">"dir"</span>) ~= 7
    unzip(localfile,downloadFolder)
<span style="color:#0000FF">end</span>
addpath(fullfile(downloadFolder,<span style="color:#A020F0">"yamnet"</span>))</pre></div></div></div><h3 class="title" id="AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-3">Read Audio Data and Classify the Sounds</h3><p>Use <a href="../ref/audioread.html" data-docid="matlab_ref#btiabil-1" class="a"><code class="olink">audioread</code></a> to read the audio file data and listen to it using <a href="../ref/sound.html" data-docid="matlab_ref#btj60o3-1" class="a"><code class="olink">sound</code></a> function.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>[audioIn, fs] = audioread(<span style="color:#A020F0">"multipleSounds-16-16-mono-18secs.wav"</span>);
sound(audioIn,fs)</pre></div></div></div><p>Call <a href="../../audio/ref/classifysound.html" data-docid="audio_ref#mw_f8a72c9b-2ca6-411a-9831-3fb82c50c6b1" class="a"><code class="olink">classifySound</code></a><span role="cross_prod"> (Audio Toolbox)</span> to detect the different sounds present in the given audio.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>detectedSounds = classifySound(audioIn,fs)</pre></div></div></div><div class="code_responsive"><div class="programlisting"><div class="codeoutput"><pre>detectedSounds = <span class="emphasis"><em>1×5 string</em></span>
    "Stream"    "Machine gun"    "Snoring"    "Bark"    "Meow"

</pre></div></div></div><p>You detected the different sounds in the pre-recorded audio in offline mode. The later sections of this example demonstrates the audio event classification in the real-time scenario where you process one audio frame at a time.</p><h3 class="title" id="AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-4">Load TensorFlow Lite Model and Audio Event Classes</h3><p>You load the TFLite YAMNet using <a href="../../deeplearning/ref/loadtflitemodel.html" data-docid="nnet_ref#mw_2d49487a-53ee-4754-83e2-58c941d21643" class="a"><code class="olink">loadTFLiteModel</code></a><span role="cross_prod"> (Deep Learning Toolbox)</span><code class="literal">.</code> As mentioned in <a href="../../deeplearning/ref/tflitemodel.html" data-docid="nnet_ref#mw_9753224d-f642-41ae-9db0-726b0ba96fbf" class="a"><code class="olink">TFLiteModel</code></a><span role="cross_prod"> (Deep Learning Toolbox)</span> page, you set the <code class="literal">Mean</code> and <code class="literal">Variance</code> parameter of the TFLite model to <code class="literal">0</code> and <code class="literal">1</code>, respectively, because the input to YAMNet is not already normalized.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>modelFileName = <span style="color:#A020F0">"lite-model_yamnet_classification_tflite_1.tflite"</span>;
modelFullPath = fullfile(downloadFolder,<span style="color:#A020F0">"yamnet"</span>,modelFileName);
TFLiteYAMNet = loadTFLiteModel(modelFullPath);
TFLiteYAMNet.Mean = 0;
TFLiteYAMNet.StandardDeviation = 1;</pre></div></div></div><p>Use <a href="../../audio/ref/yamnetgraph.html" data-docid="audio_ref#function_yamnetGraph" class="a"><code class="olink">yamnetGraph</code></a><span role="cross_prod"> (Audio Toolbox)</span> to load all the audio event classes supported by YAMNet, as an array of strings.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>[~, audioEventClasses] = yamnetGraph;</pre></div></div></div><p>Set the sample rate (in Hertz), the length of input audio frame and the frame duration in seconds, supported by YAMNet.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>modelSamplingRate = 16000;
frameDimension = TFLiteYAMNet.InputSize{1};
frameLength = frameDimension(2);
frameDuration = frameLength/modelSamplingRate;</pre></div></div></div><p>Set the <code class="literal">classificationRate</code> i.e. the number of classifications per second. As the number of hops per second must be equal to the classification rate, set the <code class="literal">hopDuration</code> to the reciprocal of <code class="literal">classificationRate</code>.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>classificationRate = 10;
hopDuration = 1/classificationRate;
hopLength = floor(modelSamplingRate*hopDuration);
overlapLength = frameLength - hopLength;</pre></div></div></div><h3 class="title" id="AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-5">Read Input Audio</h3><p>You use <code class="literal">dropdown</code> control to list the different input audio files. Use <a href="../../dsp/ref/dsp.audiofilereader-system-object.html" data-docid="dsp_ref#bsnafyt-1" class="a"><code class="olink">dsp.AudioFileReader</code></a><span role="cross_prod"> (DSP System Toolbox)</span> to read the audio file data.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>afr = dsp.AudioFileReader(<span class="live_control_container"><img tabindex="-1" alt="" src="../../examples/deeplearning_shared/win64/AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample_03.png"><span data-examplename="openExample('deeplearning_shared/AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample')" tabindex="0" data-controlvalue="&#34;multipleSounds-16-16-mono-18secs.wav&#34;" data-bs-trigger="focus" data-bs-container="body" data-bs-toggle="popover" data-bs-placement="top" data-bs-html="true" class="popover_trigger add_cursor_pointer live_control_equivalent_value">"multipleSounds-16-16-mono-18secs.wav"</span></span>);
audioInSamplingRate = afr.SampleRate;
audioFileInfo = audioinfo(afr.Filename);</pre></div></div></div><p>Set the <code class="literal">SamplesPerFrame</code> corresponding to one hop.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>audioInFrameLength = floor(audioInSamplingRate*hopDuration);
afr.SamplesPerFrame = audioInFrameLength;</pre></div></div></div><h3 class="title" id="AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-6">Setup the FIFO Buffers</h3><p>Create two <a href="../../dsp/ref/dsp.asyncbuffer-system-object.html" data-docid="dsp_ref#bvkqgmw-1" class="a"><code class="olink">dsp.AsyncBuffer</code></a><span role="cross_prod"> (DSP System Toolbox)</span> objects <code class="literal">audioBufferYamnet</code> and <code class="literal">audioClassBuffer</code> to buffer the resampled audio samples and the indices of predicted audio classes. You set the length of the <code class="literal">audioClassBuffer</code> corresponding to <code class="literal">predictedAudiolassesDuration</code> seconds. You initialize the <code class="literal">audioClassBuffer</code> with the index corresponding to the <code class="literal">Silence</code> audio class.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>predictedAudiolassesDuration = 1;
audioClassBufferLength = floor(predictedAudiolassesDuration*classificationRate);
audioClassBuffer = dsp.AsyncBuffer(audioClassBufferLength);
audioBufferYamnet = dsp.AsyncBuffer(2*frameLength);
indexOfSilenceAudioClass = find(audioEventClasses == <span style="color:#A020F0">"Silence"</span>);
write(audioClassBuffer,ones(audioClassBufferLength,1)*indexOfSilenceAudioClass);</pre></div></div></div><p>Create a <a href="../../dsp/ref/timescope.html" data-docid="dsp_ref#mw_a35c8917-4fd1-4557-b840-f27aeb1567be" class="a"><code class="olink">timescope</code></a><span role="cross_prod"> (DSP System Toolbox)</span> object to visualize the audio.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>timeScope = timescope(<span style="color:#A020F0">"SampleRate"</span>, modelSamplingRate, <span style="color:#0000FF">...</span>
    <span style="color:#A020F0">"YLimits"</span>,[-1 1], <span style="color:#0000FF">...</span>
    <span style="color:#A020F0">"Name"</span>,<span style="color:#A020F0">"Audio Event Classification Using TensorFlow Lite YAMNet"</span>, <span style="color:#0000FF">...</span>
    <span style="color:#A020F0">"TimeSpanSource"</span>,<span style="color:#A020F0">"Property"</span>, <span style="color:#0000FF">...</span>
    <span style="color:#A020F0">"TimeSpan"</span>,audioFileInfo.Duration);</pre></div></div></div><h3 class="title" id="AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-7">Run TFLite YAMNet in MATLAB to Perform Audio Event Classification</h3><p>Setup a <a href="../../dsp/ref/dsp.samplerateconverter-system-object.html" data-docid="dsp_ref#bug9vyd" class="a"><code class="olink">dsp.SampleRateConverter</code></a><span role="cross_prod"> (DSP System Toolbox)</span> system object to convert the sampling rate of the input audio to 16000 Hz, as YAMNet is trained using audio signals sampled at 16000 Hz sampling rate.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>src = dsp.SampleRateConverter(<span style="color:#A020F0">'InputSampleRate'</span>,audioInSamplingRate,<span style="color:#0000FF">...</span>
                              <span style="color:#A020F0">'OutputSampleRate'</span>,modelSamplingRate,<span style="color:#0000FF">...</span>
                              <span style="color:#A020F0">'Bandwidth'</span>,10000);</pre></div></div></div><p>You feed one audio frame at a time to represent the system as it would be deployed in a real-time embedded system. In the streaming loop, you first load one hop of audio samples and fed them to the <a href="../../dsp/ref/dsp.samplerateconverter-system-object.html" data-docid="dsp_ref#bug9vyd" class="a"><code class="olink">dsp.SampleRateConverter</code></a><span role="cross_prod"> (DSP System Toolbox)</span> to convert the sampling rate to 16000 Hz. The resampled frame is written in a FIFO buffer, <code class="literal">audioBufferYamnet</code>, you load the overlapping frames of length <code class="literal">frameLength</code> from this buffer and fed it to the YAMNet. The TensorFlow Lite YAMNet model outputs the predicted score vector that contains a score for each audio event class. You calculate the index of the maximum score in the score vector and write it in the FIFO buffer, <code class="literal">audioClassBuffer</code>. The predicted index is the statistical <a href="../ref/double.mode.html" data-docid="matlab_ref#btsadh7-1" class="a"><code class="olink">mode</code></a> of the contents of the <code class="literal">audioClassBuffer</code>. The predicted audio event class is the value of <code class="literal">audioEventClasses</code> array at the predicted index. You visualize the resampled audio frame in the time scope and print the predicted audio event class as the title of the time scope.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre><span style="color:#0000FF">while</span> ~isDone(afr)
    audioInFrame = afr();
    resampledAudioInFrame = src(audioInFrame);
    write(audioBufferYamnet,resampledAudioInFrame);
    audioInYamnetFrame = read(audioBufferYamnet,frameLength,overlapLength);
    scoresTFLite = TFLiteYAMNet.predict(audioInYamnetFrame');
    [~, audioClassIndex] = max(scoresTFLite);
    write(audioClassBuffer,audioClassIndex);
    preditedSoundClass = audioEventClasses(mode(audioClassBuffer.peek(audioClassBufferLength)));
    timeScope(resampledAudioInFrame);
    timeScope.Title = char(preditedSoundClass);
    drawnow
<span style="color:#0000FF">end</span>
hide(timeScope)
reset(timeScope)
reset(afr)</pre></div></div></div><h3 class="title" id="AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-8">Prepare MATLAB Code for Deployment</h3><p>You prepare a MATLAB function <a href="matlab:openExample('deeplearning_shared/AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample','supportingFile','predictAudioClassUsingYAMNET.m')" target="_blank"><code class="literal">predictAudioClassUsingYAMNET</code></a> that performs audio class prediction for the input audio frames. It buffers the indices of the predicted audio class in a FIFO buffer. The predicted audio class index is the statistical <a href="../ref/double.mode.html" data-docid="matlab_ref#btsadh7-1" class="a"><code class="olink">mode</code></a> of the contents of this FIFO buffer.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>type <span style="color:#A020F0">predictAudioClassUsingYAMNET.m</span></pre></div></div></div><div class="code_responsive"><div class="programlisting"><div class="codeoutput"><pre>function preditedAudioClassIndex = predictAudioClassUsingYAMNET(audioIn, audioClassHistoryBufferLength,indexSilenceAudioClass)
% predictAudioClassUsingYAMNET Predicts the audio class of input audio by
% using a pre-trained TensorFlow Lite YAMNET model.
%
% Input Arguments:
% audioIn                           - Audio frame of length 1x15600 with
%                                     sampling rate of 16000 samples per
%                                     second
% audioClassHistoryBufferLength     - Length of the audio class FIFO buffer
%                                     to contain predicted audio class
%                                     indices. The index of the predicted
%                                     audio class is the statistical mode
%                                     of the contents of this buffer.
%
% Output Arguments:
% preditedAudioClassIndex           - Index of the predicted audio class.
%
%
% Copyright 2022 The MathWorks, Inc.

%#codegen

persistent TFLiteYAMNETModel AudioClassBuffer

if isempty(TFLiteYAMNETModel)
    TFLiteYAMNETModel = loadTFLiteModel("lite-model_yamnet_classification_tflite_1.tflite");
    TFLiteYAMNETModel.NumThreads = 4;
    TFLiteYAMNETModel.Mean = 0;
    TFLiteYAMNETModel.StandardDeviation = 1;

    % Create and initialize a FIFO buffer with index of the 'Silence'
    AudioClassBuffer = dsp.AsyncBuffer(audioClassHistoryBufferLength);
    write(AudioClassBuffer,ones(audioClassHistoryBufferLength,1)*indexSilenceAudioClass);
end

scores = predict(TFLiteYAMNETModel,audioIn);
[~, audioClassIndex] = max(scores);
write(AudioClassBuffer,audioClassIndex);
predictedAudioClassHistory = peek(AudioClassBuffer,audioClassHistoryBufferLength);
preditedAudioClassIndex = mode(predictedAudioClassHistory);
end
</pre></div></div></div><h3 class="title" id="AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-9">Generate Code for Audio Event Classifier on Raspberry Pi</h3><p><strong class="emphasis bold">Create Code Generation Configuration</strong></p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>cfg = coder.config(<span style="color:#A020F0">"lib"</span>, <span style="color:#A020F0">"ecoder"</span>, true);
cfg.TargetLang = <span style="color:#A020F0">'C++'</span>;
cfg.VerificationMode = <span style="color:#A020F0">"PIL"</span>;</pre></div></div></div><p><strong class="emphasis bold">Set Up Connection with Raspberry Pi</strong></p><p>Use the Raspberry Pi Support Package function, <code class="literal">raspi</code>, to create a connection to your Raspberry Pi. In the following code, replace:</p><div class="itemizedlist"><ul><li><p><code class="literal">raspiname</code> with the name of your Raspberry Pi</p></li><li><p><code class="literal">pi</code> with your user name</p></li><li><p><code class="literal">password</code> with your password</p></li></ul></div><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre><span style="color:#0000FF">if</span> ~(exist(<span style="color:#A020F0">"r"</span>,<span style="color:#A020F0">"var"</span>))
  r = raspi(<span style="color:#A020F0">"raspiname"</span>,<span style="color:#A020F0">"pi"</span>,<span style="color:#A020F0">"password"</span>);
<span style="color:#0000FF">end</span></pre></div></div></div><p><strong class="emphasis bold">Configure Code Generation Hardware Parameters for Raspberry Pi</strong></p><p>Create a <a href="../../coder/ref/coder.hardware.html" data-docid="coder_ref#mw_c0d3d077-d33a-4dca-b85d-76027b3170b7" class="a"><code class="olink">coder.hardware</code></a><span role="cross_prod"> (MATLAB Coder)</span> object for Raspberry Pi and attach it to the code generation configuration object.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>hw = coder.hardware(<span style="color:#A020F0">"Raspberry Pi"</span>);
cfg.Hardware = hw;</pre></div></div></div><p>Specify the build folder on Raspberry Pi.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>buildDir = <span style="color:#A020F0">"~/remoteBuildDir"</span>;
cfg.Hardware.BuildDir = buildDir;</pre></div></div></div><p><strong class="emphasis bold">Copy TensorFlow Lite Model to the Target Hardware and the Current Directory</strong></p><p>Copy the TensorFlow Lite model to the Raspberry Pi board. On the hardware board, set the environment variable TFLITE_MODEL_PATH to the location of the TensorFlow Lite model. For more information on setting environment variables, see <a href="../../deeplearning/ug/prerequisites-for-deep-learning-with-tensorflow-lite-models.html" data-docid="nnet_ug#mw_ebedddaa-3331-4648-9b72-974745cdb9f5" class="a">Prerequisites for Deep Learning with TensorFlow Lite Models</a><span role="cross_prod"> (Deep Learning Toolbox)</span>.</p><p>Use <code class="literal">putFile</code> method of the <code class="literal">raspi</code> object to copy the TFLite model to Raspberry Pi.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>putFile(r,char(modelFullPath),<span style="color:#A020F0">'/home/pi'</span>)</pre></div></div></div><p>Copy the model to the current directory as it is required by <a href="../../coder/ref/codegen.html" data-docid="coder_ref#br46oyi-1" class="a"><code class="olink">codegen</code></a><span role="cross_prod"> (MATLAB Coder)</span> during code generation.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>copyfile(modelFullPath)</pre></div></div></div><p><strong class="emphasis bold">Generate PIL MEX</strong></p><p>You use <a href="../../coder/ref/coder.constant-class.html" data-docid="coder_ref#bsvczxi" class="a"><code class="olink">coder.Constant</code></a><span role="cross_prod"> (MATLAB Coder)</span> to make the constant input arguments, compile time constants in the generated code. Run the <a href="../../coder/ref/codegen.html" data-docid="coder_ref#br46oyi-1" class="a"><code class="olink">codegen</code></a><span role="cross_prod"> (MATLAB Coder)</span> command to generate a PIL MEX function <code class="literal">predictAudioClassUsingYAMNET_pil</code>. </p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>codegen <span style="color:#A020F0">-config</span> <span style="color:#A020F0">cfg</span> <span style="color:#A020F0">predictAudioClassUsingYAMNET</span> <span style="color:#A020F0">-args</span> <span style="color:#A020F0">{ones(1,15600,"single"), coder.Constant(audioClassBufferLength), coder.Constant(indexOfSilenceAudioClass)}</span> <span style="color:#A020F0">-silent</span></pre></div></div></div><div class="code_responsive"><div class="programlisting"><div class="codeoutput"><pre>### Connectivity configuration for function 'predictAudioClassUsingYAMNET': 'Raspberry Pi'
</pre></div></div></div><h3 class="title" id="AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-10">Predict Audio Class on Raspberry Pi Using PIL Workflow</h3><p>You call the generated PIL function <code class="literal">predictAudioClassUsingYAMNET_pil to</code> stream one audio frame at a time to represent the system as it would be deployed in a real-time embedded system.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>show(timeScope)
<span style="color:#0000FF">while</span> ~isDone(afr)
    audioInFrame = afr();
    resampledAudioInFrame = src(audioInFrame);
    write(audioBufferYamnet,resampledAudioInFrame);
    audioInYamnetFrame = read(audioBufferYamnet,frameLength,overlapLength);
    predictedSoundClassIndex = predictAudioClassUsingYAMNET_pil(single(audioInYamnetFrame'),audioClassBufferLength, indexOfSilenceAudioClass);
    preditedSoundClass = audioEventClasses(predictedSoundClassIndex);
    timeScope(resampledAudioInFrame)
    timeScope.Title = char(preditedSoundClass);
    drawnow
<span style="color:#0000FF">end</span></pre></div></div></div><div class="code_responsive"><div class="programlisting"><div class="codeoutput"><pre>### Starting application: 'codegen\lib\predictAudioClassUsingYAMNET\pil\predictAudioClassUsingYAMNET.elf'
    To terminate execution: clear predictAudioClassUsingYAMNET_pil
### Launching application predictAudioClassUsingYAMNET.elf...
</pre></div></div></div><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>hide(timeScope)</pre></div></div></div><div class="informalfigure"><div id="d126e123593" class="mediaobject"><p><img src="../../examples/deeplearning_shared/win64/AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample_01.png" alt="AudioEventClassificationGunFire.png" height="531" width="799"></p></div></div><p>Terminate the PIL execution</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>clear <span style="color:#A020F0">predictAudioClassUsingYAMNET_pil</span></pre></div></div></div><div class="code_responsive"><div class="programlisting"><div class="codeoutput"><pre>### Host application produced the following standard output (stdout) and standard error (stderr) messages:
</pre></div></div></div><h3 class="title" id="AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample-11">Evaluate Raspberry Pi Execution Time</h3><p>You use PIL workflow to profile the <code class="literal">predictAudioClassUsingYAMNET</code> function. You enable profiling in the code generation configuration and generate the PIL function that keeps a log of execution profile.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>cfg.CodeExecutionProfiling = true;
codegen <span style="color:#A020F0">-config</span> <span style="color:#A020F0">cfg</span> <span style="color:#A020F0">predictAudioClassUsingYAMNET</span> <span style="color:#A020F0">-args</span> <span style="color:#A020F0">{ones(1,15600,"single"), coder.Constant(audioClassBufferLength), coder.Constant(indexOfSilenceAudioClass)}</span> <span style="color:#A020F0">-silent</span></pre></div></div></div><div class="code_responsive"><div class="programlisting"><div class="codeoutput"><pre>### Connectivity configuration for function 'predictAudioClassUsingYAMNET': 'Raspberry Pi'
</pre></div></div></div><p>You call the generated PIL function multiple times to get the average execution time.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>numCalls = 100;
<span style="color:#0000FF">for</span> k = 1:numCalls
    x = pinknoise(1,15600,<span style="color:#A020F0">"single"</span>);
    scores = predictAudioClassUsingYAMNET_pil(x,audioClassBufferLength,indexOfSilenceAudioClass);
<span style="color:#0000FF">end</span></pre></div></div></div><div class="code_responsive"><div class="programlisting"><div class="codeoutput"><pre>### Starting application: 'codegen\lib\predictAudioClassUsingYAMNET\pil\predictAudioClassUsingYAMNET.elf'
    To terminate execution: clear predictAudioClassUsingYAMNET_pil
### Launching application predictAudioClassUsingYAMNET.elf...
    Execution profiling data is available for viewing. Open Simulation Data Inspector.
    Execution profiling report available after termination.
</pre></div></div></div><p>Terminate the PIL execution.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>clear <span style="color:#A020F0">predictAudioClassUsingYAMNET_pil</span> </pre></div></div></div><div class="code_responsive"><div class="programlisting"><div class="codeoutput"><pre>### Host application produced the following standard output (stdout) and standard error (stderr) messages:

    Execution profiling report: coder.profile.show(getCoderExecutionProfile('predictAudioClassUsingYAMNET'))
</pre></div></div></div><p>Generate an execution profile report to evaluate execution time.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>executionProfile = getCoderExecutionProfile(<span style="color:#A020F0">'predictAudioClassUsingYAMNET'</span>);
report(executionProfile, <span style="color:#0000FF">...</span>
       <span style="color:#A020F0">'Units'</span>,<span style="color:#A020F0">'Seconds'</span>, <span style="color:#0000FF">...</span>
       <span style="color:#A020F0">'ScaleFactor'</span>,<span style="color:#A020F0">'1e-03'</span>, <span style="color:#0000FF">...</span>
       <span style="color:#A020F0">'NumericFormat'</span>,<span style="color:#A020F0">'%0.4f'</span>);</pre></div></div></div><div class="informalfigure"><div id="d126e123627" class="mediaobject"><p><img src="../../examples/deeplearning_shared/win64/AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample_02.png" height="902" width="1606"></p></div></div><p>In the code execution profiling report, you find that the average execution time taken by <a href="matlab:openExample('deeplearning_shared/AudioEventClassificationUsingTensorFlowLiteOnRaspberryPiExample','supportingFile','predictAudioClassUsingYAMNET.m')" target="_blank"><code class="literal">predictAudioClassUsingYAMNET</code></a> is <code class="literal">24.29 ms</code> which is within the budget of <code class="literal">100 ms</code>. You calculate the budget as the reciprocal of the classification rate. The performance is measured on Raspberry Pi 3 Model B Plus Rev 1.2.</p><p>Release buffers, timescope and other system objects used in the example.</p><div class="code_responsive"><div class="programlisting"><div class="codeinput"><pre>release(audioBufferYamnet)
release(audioClassBuffer)
release(timeScope)
release(src)
release(afr)</pre></div></div></div></div></section>
    </div></section><div xmlns="http://www.w3.org/1999/xhtml" class="clearfix"></div>
<div xmlns="http://www.w3.org/1999/xhtml" align="center" class="feedbackblock" id="mw_docsurvey"><script src="https://www.mathworks.com/help/docsurvey/docfeedback.js"></script>
<script>loadSurveyHidden();</script>
<link rel="stylesheet" href="https://www.mathworks.com/help/docsurvey/release/index-css.css" type="text/css">
<script src="https://www.mathworks.com/help/docsurvey/release/bundle.index.js"></script>

<script>initDocSurvey();</script></div></main>


</div>
</div>
</div>
</div><!--close_0960-->
<footer xmlns="http://www.w3.org/1999/xhtml" id="footer" class="bs-footer">
<div class="container-fluid">
<div class="footer">
<div class="row">
<div class="col-12">
<p class="copyright">© 1994-2025 The MathWorks, Inc.</p>
<ul class="footernav"><li class="footernav_help"><a href="matlab:web(matlab.internal.licenseAgreement)">Terms of Use</a></li><li class="footernav_patents"><a href="matlab:web([matlabroot '/patents.txt'])">Patents</a></li><li class="footernav_trademarks"><a href="matlab:web([matlabroot '/trademarks.txt'])">Trademarks</a></li><li class="footernav_piracy"><a href="matlab:web([docroot '/acknowledgments.html'])">Acknowledgments</a></li></ul></div>
</div>
</div>
</div>
</footer>
</div><!--close row-offcanvas-->
</div><!--close_0970-->
</body>
</html>
